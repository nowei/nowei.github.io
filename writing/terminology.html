<!DOCTYPE html>
<html lang='en-US'>
<head>
  <title>CS terminology</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../style.css">
  <style>
  </style>
  <script src="https://code.jquery.com/jquery-3.3.1.js" integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60=" crossorigin="anonymous"></script>
  <script>
    $(function() {
      $("#header").load("../header.html");
    });
  </script>
</head>
<body>
  <div id="header"></div>
  Page for (ideally) human-readable definitions of CS topics. 
  In other words, how I would explain things to people if they asked me about something. 
  
  Reflection is important. Being able to explain things is also important, so I've decided to write down things I've learned. 
  I'm trying to write them as I learn them, but it might take a bit to catch up with what I've seen. 

  Machine Learning: 
  Model: 
  Loss Function: 
  Bias-variance tradeoffs: Bias errors refers to errors caused by model complexity (e.g. not being complex enough to learn the model or being too complex and overfitting) and variance errors refers to the errors made due to fitting to the noise in training vs. validation/test sets. Another example would be that if your model learns the training data very well, but doesn't do very well on validation or test data, then it's possible that the features chosen might help distinguish things in the training set, but may not be very helpful for the validation/test sets. 
  Kernel method: Compute phi(x)_T phi(x') with |x| many comps where phi moves |x| -> m where m &gt |x|
  Stacking: Have a bunch of models and combine outputs, then learn another model on the outputs of those models
  Bagging: 
  Cross-validation: 
  Data augmentation: 
  Transductive learning: Given a labeled training set and unlabeled test set, perform predictions only for the test points.
  Boot-strapping: 

  Deep Learning: 
  Gradient Descent: 
  Batch Normalization: 
  Epsilon Nets: 
  Universal Approximation Theorem: 
  Weak Supervision: 
  Embedding Layer: https://gdcoder.com/what-is-an-embedding-layer/
  Simulated Annealing: Approximates global optimum by defining problem as a function with many parameters, subject to constraints. Based on heating and controlled cooling to alter physical properties. In practice, the constraint can be included in the objective function as a penalty. At each step, there's a probability between moving the system to a new, neighboring state or staying in the current state. The probabilities lead the system to be in a state with lower energy. States are altered to produce neighboring states. Metaheuristics use the neighbors of a solution to explore the solution space and while preferring better neighbors, they also keep worse ones to avoid getting stuck in local optimum. The idea of temperature is how much we should still explore (sensitivity of variation), with smaller T favoring going downhill more and more and T = 0 meaning that we only go downhill. 


  Distributed Systems:
  Linearizability:
  Serializability: 
  CAP theory:
  - 
  Paxos: 

  Networking: 
  Why do packets get dropped? Switch buffers might be full [network congestion]. Receiver buffers might be full because app doesn't recv(). 

  Algorithms: 
  Principle of deferred decision: Put off deciding everything at once and instead decide things one at a time to get to the solution. 


  Computer Graphics:
  Transparent: clear
  Opaque: not clear
  Ray Tracing: 
  Gimbal Lock: 
  Quaternion: 

  Computer Vision: 

  Computer Architecture:
  Dataflow model of computation: Execute when inputs are ready
  Control flow (Von Neumann model of computation): Executes instructions in the order specified 
  https://cacm.acm.org/magazines/2019/6/237005-heterogeneous-von-neumann-dataflow-microprocessors/fulltext

  GPUs
  Bank Conflicts: Local memory is divided into memory banks, two memory locations can occur in the same bank. Each bank can only address one dataset at a time, so if a halfwarp loads/stores data from/to the same bank in an action, the access has to be serialized (bank conflict), which loses the parallel speedup. 
  Shared Nothing: each thread works by itself without looking at data from neighboring threads.
  Shared Memory: Place in GPU memory unique to a ___(Warp?)___. Usually requires a parallel copy operation but then threads in a ____ can access the data in shared memory. 

  Polyhedral Compilation: loops treated like parametric polyhedra and exploting combinatorial and geometrical optimizatoins on these objects to analyze and optimize the programs. Can be helpful in many contexts, but the main thing is that it helps by optimizing based on structures, not the number of elements the structures represent. (https://polyhedral.info/)

  Math tricks or something:
  Jensen's inequality:
  Algos majority probability thing boosting?

  <div id="footer"></div>
</body>